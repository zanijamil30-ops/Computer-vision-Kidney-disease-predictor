# -*- coding: utf-8 -*-
"""kidney tumor code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16L6n8esfW2m__-T_ERaLztFB2sz_bR4F
"""

from google.colab import drive
drive.mount('/content/drive')

DATASET_DIR = '/content/drive/MyDrive/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone'
EXPORT_DIR = '/content/drive/MyDrive/kidney_model_exports'  # where models will be saved
!mkdir -p "{EXPORT_DIR}"
print("DATASET_DIR:", DATASET_DIR)
print("EXPORT_DIR:", EXPORT_DIR)

"""Create tf.data datasets (stream from Drive)"""

import tensorflow as tf
IMG_SIZE = (224,224)
BATCH_SIZE = 16
VAL_SPLIT = 0.2
SEED = 123

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    DATASET_DIR,
    labels='inferred',
    label_mode='categorical',   # yields one-hot labels -> ensures multiclass training
    validation_split=VAL_SPLIT,
    subset='training',
    seed=SEED,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE
)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    DATASET_DIR,
    labels='inferred',
    label_mode='categorical',
    validation_split=VAL_SPLIT,
    subset='validation',
    seed=SEED,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE
)

class_names = train_ds.class_names
print("Classes:", class_names)
AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.prefetch(AUTOTUNE)
val_ds = val_ds.prefetch(AUTOTUNE)

"""Model build (EfficientNetB0 transfer learning)"""

from tensorflow.keras import layers, Model
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.optimizers import Adam

num_classes = len(class_names)
print("num_classes:", num_classes)

data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.05),
    layers.RandomZoom(0.05),
])

inputs = layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))
x = data_augmentation(inputs)
x = layers.Rescaling(1./255)(x)
base = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))
base.trainable = False
x = base(x, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.3)(x)
outputs = layers.Dense(num_classes, activation='softmax')(x)   # <-- force softmax multiclass
model = Model(inputs, outputs)

model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

"""Callbacks"""

import os
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping

os.makedirs(EXPORT_DIR, exist_ok=True)
ckpt_path = os.path.join(EXPORT_DIR, 'kidney_best.h5')
checkpoint = ModelCheckpoint(ckpt_path, save_best_only=True, monitor='val_loss', mode='min')
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)
early = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1)
print("Checkpoint will be saved to:", ckpt_path)

"""Train (initial)"""

EPOCHS = 8   # start small, increase if you want
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    callbacks=[checkpoint, reduce_lr, early]
)

"""Fine-tune"""

# Unfreeze some layers and fine-tune
base.trainable = True
fine_tune_at = len(base.layers) - 20
for layer in base.layers[:fine_tune_at]:
    layer.trainable = False

model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])
history_fine = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=6,   # extra fine-tune epochs
    callbacks=[checkpoint, reduce_lr, early]
)

"""Verify output shape and test single prediction"""

import numpy as np
from tensorflow.keras.preprocessing import image
from PIL import Image
# pick a sample from val_ds (one batch)
for imgs, labels in val_ds.take(1):
    sample = imgs[0]     # tensor
    break

arr = tf.keras.preprocessing.image.img_to_array(sample) / 255.0
arr = np.expand_dims(arr, 0)   # shape (1, H, W, 3)
pred = model.predict(arr)
print("pred.shape:", np.asarray(pred).shape)
print("pred (first):", pred[0])
# ensure number of probs equals num_classes
assert np.asarray(pred).shape[1] == num_classes, "Model output size mismatch!"
print("Output shape OK -> multiclass softmax (no index errors expected).")

"""Save model artifacts & class names"""

import os, json
import numpy as np
import tensorflow as tf
from tensorflow import keras

os.makedirs(EXPORT_DIR, exist_ok=True)

# 1) Export TF SavedModel directory (use model.export in Keras 3)
saved_tf_dir = os.path.join(EXPORT_DIR, 'kidney_saved_model')
try:
    # Preferred (Keras 3)
    if hasattr(model, "export"):
        model.export(saved_tf_dir)
        print("Exported SavedModel (model.export) to:", saved_tf_dir)
    else:
        # Fallback to tf.saved_model.save (works for TF objects)
        tf.saved_model.save(model, saved_tf_dir)
        print("Saved SavedModel (tf.saved_model.save) to:", saved_tf_dir)
except Exception as e:
    print("Could not export SavedModel directory. Exception:")
    print(e)

# 2) Save as single-file native Keras format (.keras) - recommended
keras_path = os.path.join(EXPORT_DIR, 'kidney_model_best.keras')
try:
    model.save(keras_path)   # Keras infers format from .keras extension
    print("Saved native Keras file to:", keras_path)
except Exception as e:
    print("Could not save .keras file. Exception:")
    print(e)

# 3) Save class names
class_names_path = os.path.join(EXPORT_DIR, 'class_names.json')
with open(class_names_path, 'w') as f:
    json.dump(class_names, f, indent=2)
print("Saved class_names.json to:", class_names_path)

# 4) Simple verification: reload the single-file Keras model and predict on a sample
# Note: this assumes `sample` exists in the notebook (from your test block). If not, skip.
try:
    if os.path.exists(keras_path):
        reloaded = keras.models.load_model(keras_path)
        print("Reloaded model from .keras file successfully.")

        # Prepare an example input: use 'sample' tensor if available, else try one batch from val_ds
        try:
            img_arr = None
            # prefer 'sample' variable from earlier
            if 'sample' in globals():
                img = sample.numpy()  # sample is a tensor (H,W,3) or (batch, H,W,3)
                if img.ndim == 4:    # batch present
                    img = img[0]
                img_arr = np.expand_dims(img / 255.0, axis=0)
            else:
                # fallback: take one batch from val_ds
                for imgs, labels in val_ds.take(1):
                    img = imgs[0].numpy()
                    img_arr = np.expand_dims(img / 255.0, axis=0)
                    break

            if img_arr is not None:
                preds = reloaded.predict(img_arr)
                print("Quick predict OK. preds shape:", np.asarray(preds).shape)
            else:
                print("No sample image available for quick predict; skipping prediction.")
        except Exception as e:
            print("Reloaded model but prediction failed. Exception:")
            print(e)
    else:
        print("No .keras file found to reload; skipping verification.")
except Exception as e:
    print("Verification step failed. Exception:")
    print(e)

"""Download H5 to local machine from Colab"""

# Save and download H5 from Colab (robust: installs h5py if needed)
import os
import sys
import json
import traceback

EXPORT_DIR = os.path.expanduser(EXPORT_DIR) if 'EXPORT_DIR' in globals() else '/content'
os.makedirs(EXPORT_DIR, exist_ok=True)

h5_path = os.path.join(EXPORT_DIR, 'kidney_model_best.h5')

def try_save_h5(model, path):
    try:
        model.save(path)  # Keras infers HDF5 from .h5 extension
        print(f"Saved .h5 to: {path}")
        return True
    except Exception as e:
        print("Initial attempt to save .h5 failed with exception:")
        traceback.print_exc()
        # Detect common missing-h5py situation and try to install it
        err_str = str(e).lower()
        if 'h5py' in err_str or 'hdf5' in err_str or 'no module named h5py' in err_str:
            print("\nIt looks like h5py is missing. Installing h5py and retrying...")
            try:
                import subprocess
                subprocess.check_call([sys.executable, "-m", "pip", "install", "h5py"])
                print("h5py installed. Retrying save...")
                model.save(path)
                print(f"Saved .h5 to: {path}")
                return True
            except Exception as e2:
                print("Retry with h5py install also failed. Exception:")
                traceback.print_exc()
                return False
        else:
            print("The error does not look like missing h5py; not attempting install.")
            return False

# 1) Try saving .h5 (will install h5py if needed)
if 'model' not in globals():
    raise RuntimeError("No 'model' object found in the notebook. Run / build your model first.")

ok = try_save_h5(model, h5_path)

# 2) If saving failed, fallback: save .keras file (single-file Keras v3 format) then offer to download that instead
keras_path = os.path.join(EXPORT_DIR, 'kidney_model_best.keras')
if not ok:
    try:
        print("\nSaving fallback .keras single-file model...")
        model.save(keras_path)
        print("Saved native Keras file to:", keras_path)
    except Exception as e:
        print("Fallback saving .keras also failed. Exception:")
        traceback.print_exc()

# 3) Download whichever file exists (prefer .h5 when available)
from google.colab import files

if os.path.exists(h5_path):
    print("\nStarting browser download of .h5 file...")
    files.download(h5_path)
elif os.path.exists(keras_path):
    print("\n.h5 not available â€” downloading .keras file instead...")
    files.download(keras_path)
else:
    # Nothing to download
    print("\nNo export file found to download. You can inspect EXPORT_DIR:", EXPORT_DIR)
    print("Files in export dir:", os.listdir(EXPORT_DIR))